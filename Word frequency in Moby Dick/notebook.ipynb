{"nbformat_minor":2,"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"version":"3.5.2","name":"python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","file_extension":".py","mimetype":"text/x-python"}},"cells":[{"metadata":{"run_control":{"frozen":true},"dc":{"key":"3"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 1. Tools for text processing\n<p><img style=\"float: right ; margin: 5px 20px 5px 10px; width: 45%\" src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_38/img/Moby_Dick_p510_illustration.jpg\"> </p>\n<p>What are the most frequent words in Herman Melville's novel Moby Dick and how often do they occur?</p>\n<p>In this notebook, we'll scrape the novel <em>Moby Dick</em> from the website <a href=\"https://www.gutenberg.org/\">Project Gutenberg</a> (which contains a large corpus of books) using the Python package <code>requests</code>. Then we'll extract words from this web data using <code>BeautifulSoup</code>. Finally, we'll dive into analyzing the distribution of words using the Natural Language ToolKit (<code>nltk</code>). </p>\n<p>The <em>Data Science pipeline</em> we'll build in this notebook can be used to visualize the word frequency distributions of any novel that you can find on Project Gutenberg. The natural language processing tools used here apply to much of the data that data scientists encounter as a vast proportion of the world's data is unstructured data and includes a great deal of text.</p>\n<p>Let's start by loading in the three main python packages we are going to use.</p>"},{"metadata":{"collapsed":true,"dc":{"key":"3"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# Importing requests, BeautifulSoup and nltk\n# ... YOUR CODE FOR TASK 1 ...\nimport requests\nimport nltk\nfrom bs4 import BeautifulSoup\n","execution_count":1,"outputs":[]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"10"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 2. Request Moby Dick\n<p>To analyze Moby Dick, we need to get the contents of Moby Dick from <em>somewhere</em>. Luckily, the text is freely available online at Project Gutenberg as an HTML file: https://www.gutenberg.org/files/2701/2701-h/2701-h.htm .</p>\n<p><strong>Note</strong> that HTML stands for Hypertext Markup Language and is the standard markup language for the web.</p>\n<p>To fetch the HTML file with Moby Dick we're going to use the <code>request</code> package to make a <code>GET</code> request for the website, which means we're <em>getting</em> data from it. This is what you're doing through a browser when visiting a webpage, but now we're getting the requested page directly into python instead. </p>"},{"metadata":{"dc":{"key":"10"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# Getting the Moby Dick HTML \nr = requests.get(\"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\")\n\n# Setting the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n\n# Extracting the HTML from the request object\nhtml = r.text\n\n# Printing the first 2000 characters in html\n# ... YOUR CODE FOR TASK 3 ...\nhtml[:2000]","execution_count":2,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\r\\n\\r\\n<!DOCTYPE html\\r\\n   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\\r\\n   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\\r\\n\\r\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\\r\\n  <head>\\r\\n    <title>\\r\\n      Moby Dick; Or the Whale, by Herman Melville\\r\\n    </title>\\r\\n    <style type=\"text/css\" xml:space=\"preserve\">\\r\\n\\r\\n    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\\r\\n    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\\r\\n    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\\r\\n    hr  { width: 50%; text-align: center;}\\r\\n    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\\r\\n    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\\r\\n    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\\r\\n    .toc       { margin-left: 10%; margin-bottom: .75em;}\\r\\n    .toc2      { margin-left: 20%;}\\r\\n    div.fig    { display:block; margin:0 auto; text-align:center; }\\r\\n    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\\r\\n    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\\r\\n    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\\r\\n    .pagenum   {display:inline; font-size: 70%; font-style:normal;\\r\\n               margin: 0; padding: 0; position: absolute; right: 1%;\\r\\n               text-align: right;}\\r\\n    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\\r\\n\\r\\n    table      {margin-left: 10%;}\\r\\n\\r\\na:link {color:blue;\\r\\n\\t\\ttext-decoration:none}\\r\\nlink {color:blue;\\r\\n\\t\\ttext-decoration:none}\\r\\na:visited {color:blue;\\r\\n\\t\\ttext-decoration:none}\\r\\na:hover {color:red}\\r\\n\\r\\n</style>\\r\\n  </head>\\r\\n  <body>\\r\\n<pre xml:space=\"preserve\">\\r\\n\\r\\nThe Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\\r\\n\\r\\nThis eBook is for the use of anyone anywh'"},"execution_count":2}]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"17"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 3. Get the text from the HTML\n<p>This HTML is not quite what we want. However, it does <em>contain</em> what we want: the text of <em>Moby Dick</em>. What we need to do now is <em>wrangle</em> this HTML to extract the text of the novel. For this we'll use the package <code>BeautifulSoup</code>.</p>\n<p>Firstly, a word on the name of the package: Beautiful Soup? In web development, the term \"tag soup\" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called <code>BeautifulSoup</code>. After creating the soup, we can use its <code>.get_text()</code> method to extract the text.</p>"},{"metadata":{"dc":{"key":"17"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# Creating a BeautifulSoup object from the HTML\nsoup = BeautifulSoup(html)\n\n# Getting the text out of the soup\ntext = soup.get_text()\n\n# Printing out text between characters 32000 and 34000\n# ... YOUR CODE FOR TASK 3 ...\ntext[32000:34000]","execution_count":3,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'tly over him, and probably killed him in a moment.”\\n         —“The Whale and his Captors, or The Whaleman’s Adventures and the\\n        Whale’s Biography, gathered on the Homeward Cruise of the Commodore\\n        Preble.” By Rev. Henry T. Cheever.\\n      \\n      \\n        “If you make the least damn bit of noise,” replied Samuel, “I will send\\n        you to hell.” —Life of Samuel Comstock (the mutineer), by his\\n        brother, William Comstock. Another Version of the whale-ship Globe\\n        narrative.\\n      \\n      \\n        “The voyages of the Dutch and English to the Northern Ocean, in order,\\n        if possible, to discover a passage through it to India, though they\\n        failed of their main object, laid-open the haunts of the whale.” —McCulloch’s\\n        Commercial Dictionary.\\n      \\n      \\n        “These things are reciprocal; the ball rebounds, only to bound forward\\n        again; for now in laying open the haunts of the whale, the whalemen seem\\n        to have indirectly hit upon new clews to that same mystic North-West\\n        Passage.” —From “Something” unpublished.\\n      \\n      \\n        “It is impossible to meet a whale-ship on the ocean without being struck\\n        by her near appearance. The vessel under short sail, with look-outs at\\n        the mast-heads, eagerly scanning the wide expanse around them, has a\\n        totally different air from those engaged in regular voyage.” —Currents\\n        and Whaling. U.S. Ex. Ex.\\n      \\n      \\n        “Pedestrians in the vicinity of London and elsewhere may recollect\\n        having seen large curved bones set upright in the earth, either to form\\n        arches over gateways, or entrances to alcoves, and they may perhaps have\\n        been told that these were the ribs of whales.” —Tales of a Whale\\n        Voyager to the Arctic Ocean.\\n      \\n      \\n        “It was not till the boats returned from the pursuit of these whales,\\n        that the whites saw their ship in bloody possession of the savages\\n        enrolled am'"},"execution_count":3}]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"24"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 4. Extract the words\n<p>We now have the text of the novel! There is some unwanted stuff at the start and some unwanted stuff at the end. We could remove it, but this content is so much smaller in amount than the text of Moby Dick that, to a first approximation, it is okay to leave it in.</p>\n<p>Now that we have the text of interest, it's time to count how many times each word appears, and for this we'll use <code>nltk</code> – the Natural Language Toolkit. We'll start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) and then split the text into a list of words.</p>"},{"metadata":{"dc":{"key":"24"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# Creating a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenizing the text\ntokens = tokenizer.tokenize(text)\n\n# Printing out the first 8 words / tokens \n# ... YOUR CODE FOR TASK 4 ...\ntokens[:8]","execution_count":5,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['Moby', 'Dick', 'Or', 'the', 'Whale', 'by', 'Herman', 'Melville']"},"execution_count":5}]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"31"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 5. Make the words lowercase\n<p>OK! We're nearly there. Note that in the above 'Or' has a capital 'O' and that in other places it may not, but both 'Or' and 'or' should be counted as the same word. For this reason, we should build a list of all words in <em>Moby Dick</em> in which all capital letters have been made lower case.</p>"},{"metadata":{"dc":{"key":"31"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# A new list to hold the lowercased words\nwords = []\n\n# Looping through the tokens and make them lower case\nfor word in tokens:\n    ... # YOUR CODE FOR TASK 5 ...\n    words.append(word.lower())\n\n# Printing out the first 8 words / tokens \n# ... YOUR CODE FOR TASK 5 ...\nwords[:8]","execution_count":6,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']"},"execution_count":6}]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"38"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 6. Load in stop words\n<p>It is common practice to remove words that appear a lot in the English language such as 'the', 'of' and 'a' because they're not so interesting. Such words are known as <em>stop words</em>. The package <code>nltk</code> includes a good list of stop words in English that we can use.</p>"},{"metadata":{"dc":{"key":"38"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# Getting the English stop words from nltk\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\n\n# Printing out the first eight stop words\n# ... YOUR CODE FOR TASK 6 ...\nsw[:8]","execution_count":9,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']"},"execution_count":9}]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"45"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 7. Remove stop words in Moby Dick\n<p>We now want to create a new list with all <code>words</code> in Moby Dick, except those that are stop words (that is, those words listed in <code>sw</code>). One way to get this list is to loop over all elements of <code>words</code> and add each word to a new list if they are <em>not</em> in <code>sw</code>.</p>"},{"metadata":{"dc":{"key":"45"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# A new list to hold Moby Dick with No Stop words\nwords_ns = []\n\n# Appending to words_ns all words that are in words but not in sw\nfor word in words:\n    ... # YOUR CODE FOR TASK 7 ...\n    if word not in sw:\n        words_ns.append(word)\n        \n\n# Printing the first 5 words_ns to check that stop words are gone\n# ... YOUR CODE FOR TASK 7 ...\nwords_ns[:5]","execution_count":10,"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['moby', 'dick', 'whale', 'herman', 'melville']"},"execution_count":10}]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"52"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 8. We have the answer\n<p>Our original question was:</p>\n<blockquote>\n  <p>What are the most frequent words in Herman Melville's novel Moby Dick and how often do they occur?</p>\n</blockquote>\n<p>We are now ready to answer that! Let's create a word frequency distribution plot using <code>nltk</code>. </p>"},{"metadata":{"dc":{"key":"52"},"tags":["sample_code"],"trusted":true},"cell_type":"code","source":"# This command display figures inline\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Creating the word frequency distribution\nfreqdist = nltk.FreqDist(words_ns)\n\n# Plotting the word frequency distribution\n# ... YOUR CODE FOR TASK 8 ...\nplt.plot(freqdist.most_common(25), freqdist.most_common(25).values())\nplt.show()","execution_count":22,"outputs":[{"output_type":"error","traceback":["---------------------------------------------------------------------------","AttributeError                            Traceback (most recent call last)","<ipython-input-22-50058a0bcebb> in <module>()\n      8 # Plotting the word frequency distribution\n      9 # ... YOUR CODE FOR TASK 8 ...\n---> 10 plt.plot(freqdist.most_common(25).keys(), freqdist.most_common(25).values())\n     11 plt.show()\n","AttributeError: 'list' object has no attribute 'keys'"],"evalue":"'list' object has no attribute 'keys'","ename":"AttributeError"}]},{"metadata":{"dc":{"key":"52"},"trusted":true},"cell_type":"code","source":"import pandas as pd\npd.DataFrame(freqdist)","execution_count":29,"outputs":[{"output_type":"error","traceback":["---------------------------------------------------------------------------","ValueError                                Traceback (most recent call last)","<ipython-input-29-1e8d2f5175e3> in <module>()\n      1 import pandas as pd\n----> 2 pd.DataFrame(freqdist)\n","/var/lib/python/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)\n    264                                  dtype=dtype, copy=copy)\n    265         elif isinstance(data, dict):\n--> 266             mgr = self._init_dict(data, index, columns, dtype=dtype)\n    267         elif isinstance(data, ma.MaskedArray):\n    268             import numpy.ma.mrecords as mrecords\n","/var/lib/python/site-packages/pandas/core/frame.py in _init_dict(self, data, index, columns, dtype)\n    400             arrays = [data[k] for k in keys]\n    401 \n--> 402         return _arrays_to_mgr(arrays, data_names, index, columns, dtype=dtype)\n    403 \n    404     def _init_ndarray(self, values, index, columns, dtype=None, copy=False):\n","/var/lib/python/site-packages/pandas/core/frame.py in _arrays_to_mgr(arrays, arr_names, index, columns, dtype)\n   5407     # figure out the index, if necessary\n   5408     if index is None:\n-> 5409         index = extract_index(arrays)\n   5410     else:\n   5411         index = _ensure_index(index)\n","/var/lib/python/site-packages/pandas/core/frame.py in extract_index(data)\n   5446 \n   5447         if not indexes and not raw_lengths:\n-> 5448             raise ValueError('If using all scalar values, you must pass'\n   5449                              ' an index')\n   5450 \n","ValueError: If using all scalar values, you must pass an index"],"evalue":"If using all scalar values, you must pass an index","ename":"ValueError"}]},{"metadata":{"dc":{"key":"52"},"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"run_control":{"frozen":true},"dc":{"key":"59"},"tags":["context"],"deletable":false,"editable":false},"cell_type":"markdown","source":"## 9. The most common word\n<p>Nice! The frequency distribution plot above is the answer to our question. </p>\n<p>The natural language processing skills we used in this notebook are also applicable to much of the data that Data Scientists encounter as the vast proportion of the world's data is unstructured data and includes a great deal of text. </p>\n<p>So, what word turned out to (<em>not surprisingly</em>) be the most common word in Moby Dick?</p>"},{"metadata":{"collapsed":true,"dc":{"key":"59"},"tags":["sample_code"],"trusted":false},"cell_type":"code","source":"# What's the most common word in Moby Dick?\nmost_common_word = 'horse'","execution_count":0,"outputs":[]}],"nbformat":4}